<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Kinetics: Rethinking Test-Time Scaling Laws">
  <meta property="og:title" content="Kinetics"/>
  <meta property="og:description" content="Kinetics: Rethinking Test-Time Scaling Laws"/>
  <meta property="og:url" content="https://github.com/Infini-AI-Lab/Kinetics/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/icons/Bolt.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Kinetics">
  <meta name="twitter:description" content="Kinetics: Rethinking Test-Time Scaling Laws">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/icons/Kinetics.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Speculative Decoding">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Kinetics: Rethinking Test-Time Scaling Laws</title>
  <link rel="icon" type="image/x-icon" href="static/images/icons/Bolt.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
  <style>
    @font-face {
      font-family: 'TriForceFont';
      src: url('static/Triforce.ttf') format('truetype');
    }
  
    .custom-font {
      font-family: 'TriForceFont', sans-serif !important;
        font-size: 3.0rem;
    }

    .hero.is-light {
        background-color: #ffffff !important;
    }

    body {
        background-color: #ffffff; /* Plain white background */
    }

    .image-container {
        background-color: #ffffff; /* Match the new plain white */
        display: inline-block;
    }

    .image-container img {
        mix-blend-mode: multiply;
        max-width: 100%;
        height: auto;
    }


    .container.is-fluid {
      margin-left: 15px;
      margin-right: 15px;
      max-width: none;
    }
    
    .hero .hero-body {
      padding: 3rem 0;
    }
    
    .section {
      padding: 3rem 0;
    }
    
    .column.is-full-width {
      padding: 0 15px;
    }
  </style>
</head>
<body>


<!-- Section: Header Titlepage -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-fluid">
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
            <img src="static/images/icons/Bolt.png" alt="Magic Wand Icon" style="display: inline; height: 3rem; vertical-align: top;">
            <h1 class="title is-2 publication-title" style="display: inline;">Kinetics: Rethinking Test-Time Scaling Laws</h1>
            
            <br><br>
            
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://ranonrkm.github.io/" target="_blank">Ranajoy Sadhukhan</a><sup>*</sup>,</span>
              <span class="author-block"><a href="https://dreaming-panda.github.io/" target="_blank">Zhuoming Chen</a><sup>*</sup>,</span>
              <span class="author-block"><a href="http://zhenghaizhong.com/" target="_blank">Haizhong Zheng</a>,</span>
              <span class="author-block"><a href="" target="_blank">Yang Zhou</a>,</span>
              <span class="author-block"><a href="https://strubell.github.io/" target="_blank">Emma Strubell</a>,</span>
              <span class="author-block"><a href="https://www.andrew.cmu.edu/user/beidic/" target="_blank">Beidi Chen</a></span>
            </div>
            
            <div class="is-size-5 publication-authors">
              <span class="affliation">
                <small>
                  Carnegie Mellon University
                </small>
              </span>
              <span class="eql-cntrb">
                <small><br><sup>*</sup>Indicates Equal Contribution</small>
              </span>
            </div>
            
            <div class="column has-text-centered">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2408.11049" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href="https://github.com/Infini-AI-Lab/Kinetics" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              
              <!-- <span class="link-block">
                <a href="https://youtu.be/vRAaAyjr6Jo" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-youtube"></i></span>
                  <span>Video</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<!-- Section: Paper abstract -->
<section class="section hero is-light">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              <strong style="font-weight: 900;color: #0f598a">TL;DR:</strong> We introduce <strong>Kinetics</strong>, which challenges the traditional test-time scaling (TTS) laws by adopting a <strong>practical efficiency</strong> perspective. It reveals that prior compute-optimal approaches overlook major <em>key-value memory access bottlenecks</em> in various TTS strategies. By jointly considering memory and compute, the <strong>Kinetics scaling law</strong> shows that <em>it is more efficient to scale model size up to a threshold before investing more compute in test-time scaling</em>. Additionally, Kinetics promotes <strong>sparse attention</strong> to establish a better trade-off between accuracy and inference-scaling cost (both memory and compute).
            </p>
          </div>
        </div>
      </div>
    </div>
</section>


<!-- Section: Paper abstract -->
<section class="section hero is-light">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
          <h2 class="title is-3" style="text-align: center;">
            <img src="static/images/icons/Llama.png" style="height: 43px; display: inline; vertical-align:text-top;"/>
            &nbsp; Introduction
          </h2>
          <div class="content has-text-justified">

            <div style="background: #ffffff; border-left: 4px solid #4b6cb7; border-radius: 6px; padding: 16px 24px; font-size: 16px; line-height: 1.6; color: #333;">
              <p>
                The existing compute-optimal test-time scaling law usually favors small models with more test-time compute. We rethink these scaling laws from a <em style="color: #4b6cb7;">practical efficiency</em> perspective, revealing that the effectiveness of smaller models is <strong style="color: #4b6cb7;">significantly overestimated</strong>. Prior work, grounded in compute-optimality, overlooks critical <strong style="color: #222;">memory access bottlenecks</strong> introduced by inference-time strategies (e.g., Best-of-N, long chains of thought). 
              </p>
              <div style="display: flex; gap: 24px; align-items: flex-start; flex-wrap: wrap; margin-top: 16px;">
                <div style="flex: 1 1 55%;">
                  <h4 class="title is-5">
                    <img src="static/images/icons/Bolt.png" style="height: 32px; display: inline; vertical-align: middle;"/>
                    &nbsp;Kinetics Scaling Law
                  </h4>
                  <p>
                    Our holistic analysis, spanning models from <strong>0.6B to 32B parameters</strong>, introduces a new 
                    <strong><em style="color: #4b6cb7;">Kinetics Scaling Law</em></strong> that guides resource allocation by 
                    accounting for both computation and memory access. Unlike existing TTS law, we reach the following conclusion:
                  </p>
                  <div style="margin-top: 6px; padding: 6px 10px; background-color: #e8f0fe; border-left: 4px solid #4b6cb7; border-radius: 4px; font-weight: 500;">
                    TTS compute is better allocated toward larger model sizes rather than more generation tokens until a certain parameter threshold (<strong>14B</strong> for Qwen3 series) is reached.
                  </div>
                </div>
                <figure style="flex: 1 1 20%; margin: 0;">
                  <img src="static/images/combined_figure2.jpg" alt="Combined Figure 2" style="width: 100%; height: auto; border-radius: 6px; box-shadow: 0 2px 6px rgba(0,0,0,0.05);">
                  <figcaption style="text-align: center; margin-top: 8px; font-size: 14px; color: #555;">
                    <em>Figure 1: On the AIME24 dataset, the optimal Qwen3 model size wrt the existing scaling law could be upto 3x costlier than the optimal model size wrt the Kinetics scaling law.</em>
                  </figcaption>
                </figure>
              </div>
              
            
              <div style="display: flex; gap: 24px; align-items: flex-start; flex-wrap: wrap; margin-top: 16px;">
                <div style="flex: 1 1 55%;">
                  <h4 class="title is-5">
                    <img src="static/images/icons/ChatGPT_sparse.png" style="height: 32px; display: inline; vertical-align: middle;"/>
                    &nbsp;Sparse Scaling Law
                  </h4>
                  <p>
                    KV memory footprint becomes a major bottleneck for TTS. We observe that <strong style="color: #4b6cb7;">sparse attention</strong> can achieve better accuracy-cost trade-off by reducing per-token cost and enabling <em style="color: #27ae60;">longer generation lengths</em> and <em style="color: #27ae60;">more parallel samples</em> under the same cost budget. Empirically, it yields up to <strong style="color: #27ae60;">60-point</strong> gains in low-cost settings and <strong style="color: #27ae60;">5-point</strong> gains in high-cost regimes on benchmarks like <em>AIME</em> and <em>LiveCodeBench</em>. By ameliorating the KV memory access bottleneck, sparse attention can potentially reshape the optimal pareto frontier of TTS. 
                  </p>

                  <p>
                    <span style="display: inline-block; margin-top: 6px; padding: 6px 10px; background-color: #e8f0fe; border-left: 4px solid #4b6cb7; border-radius: 4px; font-weight: 500;">Sparse attention emerges as a key enabler for test-time efficiency and can potentially reshape the dense TTS landscape.</span>
                  </p>
                </div>
                <figure style="flex: 1 1 20%; margin: 0;">
                  <img src="static/images/aime24_acc_vs_trials.jpg" alt="AIME24 Accuracy vs Trials" style="width: 100%; height: auto; border-radius: 6px; box-shadow: 0 2px 6px rgba(0,0,0,0.05);">
                  <figcaption style="text-align: center; margin-top: 8px; font-size: 14px; color: #555;">
                    <em>Figure 2: Kinetics Sparse scaling law for Qwen3 series on AIME24 dataset. Here we consider the ideal case of top-k attention with no search overhead.</em>
                  </figcaption>
                </figure>
              </div>
            </div>
            
          
              <!-- <div style="display: flex; flex-wrap: wrap; justify-content: center;">
                <div style="width: 45%; min-width: 150px; margin: 5px;">
                    <canvas id="chart1"></canvas>
                </div>
                <div style="width: 45%; min-width: 150px; margin: 5px;">
                    <canvas id="chart2"></canvas>
                </div>
              </div>
              <script src="static/js/plots/throughput_latency_smaller.js"></script>
              <br> -->
            
          </div>
        </div>
      </div>
    </div>
</section>

<!-- Section: Motivation -->
<section class="section hero is-light">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
          <h2 class="title is-3" style="text-align: center;">
            <img src="static/images/icons/Idea.png" style="height: 50px; display: inline; vertical-align: middle;"/>
            &nbsp; Rethinking Test-Time Scaling Law
          </h2>
          <div class="content has-text-justified">

            <h4 class="title is-5">
              <img src="static/images/icons/MathematicsCompass.png" style="height: 36px; display: inline; vertical-align: middle;"/>
              &nbsp;A Holistic Cost Model for TTS
            </h4>
            <!-- <p>Given a speculation length γ for a sequence of length S and batch size B, let T<sub>T</sub>(B, S, 1) and T<sub>D</sub>(B, S, 1) denote the target and draft decoding latencies, respectively. The verification time, T<sub>V</sub>(B, S, γ), is the time it takes the target model to verify the γ speculated tokens in a single forward pass. Given an acceptance rate α ∈ [0, 1] and speculation length γ, Ω(γ, α) represents the expected number of tokens generated in one verification step, as described by <a style="color: #209CEE" href="https://arxiv.org/abs/2211.17192">Leviathan et al.</a>:</p> -->
            <p>
              As a first step, we revisit the cost model to understand the relative importance of compute and memory in TTS and how it can affect the choice of optimal model size and TTS configurations like chain-of-thought (CoT) length or number of trials.
            </p>
            
            <details style="margin-bottom: 24px; border: 1px solid #ddd; border-radius: 6px; padding: 12px 16px; background-color: #f9f9fb;">
              <summary style="cursor: pointer; font-weight: 600; font-size: 16px; color: #2a63b2;">
                View Full Cost Model Derivation
              </summary>
            <div style="background: #ffffff; border-radius: 6px; padding: 24px; font-size: 16px; line-height: 1.6; color: #333;">
              <p>
                We analyze test-time inference cost using a model that combines both compute and memory access, and define an equivalent cost function using <strong>eFLOPs</strong>, that unifies both kinds of costs in a single metric.
              </p>
              
              <p><strong>Definitions:</strong></p>
              <ul style="list-style-type: none; padding-left: 0; line-height: 1.6;">
                <li><strong>P</strong>: model parameter size</li>
                <li><strong>N</strong>: number of reasoning trials</li>
                <li><strong>L<sub>in</sub></strong>: input (prefix) length</li>
                <li><strong>L<sub>out</sub></strong>: outout (generation) length</li>
                <li><strong>D</strong>: attention head dimension</li>
                <li><strong>r</strong>: GQA group size</li>
                <li><strong>I</strong>: arithmetic intensity of hardware (FLOPs per GB/s bandwidth)</li>
              </ul>
              <hr style="margin: 24px 0; border: none; border-top: 1px solid #ccc;">

              <div style="background: #ffffff; padding: 24px; font-size: 16px; line-height: 1.6; color: #333;">

                <p><strong>Computation Cost:</strong></p>
                <p style="text-align: center; font-size: 18px;">
                  <span style="color: #2a63b2;">C<sub>comp</sub> = C<sub>param-compute</sub></span> + 
                  <span style="color: #2a63b2;">C<sub>attn-compute</sub></span> = 
                  <span style="color: #2a63b2;">2PNL<sub>out</sub></span> + 
                  <span style="color: #2a63b2;">(2rNL<sub>in</sub>DL<sub>out</sub> + rNDL<sub>out</sub><sup>2</sup>)</span>
                </p>
              
                <p><strong>Memory Access Cost:</strong></p>
                <p style="text-align: center; font-size: 18px;">
                  <span style="color: #20704c;">C<sub>mem</sub> = C<sub>param-mem</sub></span> + 
                  <span style="color: #20704c;">C<sub>attn-mem</sub></span> = 
                  <span style="color: #20704c;">2PL<sub>out</sub></span> + 
                  <span style="color: #20704c;">(2L<sub>in</sub>DL<sub>out</sub> + NDL<sub>out</sub><sup>2</sup>)</span>
                </p>
              
                <p>
                  Also, we consider the prompt KV cache is reused across <em>N</em> reasoning trials.
                </p>
              
                <h5 class="title is-5">eFLOPs (Equivalent FLOPs)</h5>
                <p>eFLOPs scales memory cost by the arithmetic intensity of the hardware to unify compute and memory costs in a single metric.</p>
                <div style="text-align: center; font-size: 18px; padding: 12px 0;">
                  eFLOPs = <span style="color: #2a63b2;">C<sub>comp</sub></span> + 
                  <span style="color: #20704c;">I × C<sub>mem</sub></span>
                </div>
                <p>
                  where <strong>I</strong> is the arithmetic intensity of the hardware (e.g., 562.5 for NVIDIA B200).
                </p>
              
                <h5 class="title is-5">Final Cost Model:</h5>
                <p>
                  In real-world deployment, model parameters are amortized across large batches, rendering 
                  <span style="color: #20704c;">C<sub>param-mem</sub></span> insignificant.
                </p>
                <div style="text-align: center; font-size: 18px; padding: 12px 0;">
                  C<sub>TTS</sub> = 
                  <span style="color: #2a63b2;">2NPL<sub>out</sub> + 2rNL<sub>in</sub>DL<sub>out</sub> + rNDL<sub>out</sub><sup>2</sup></span> + 
                  <span style="color: #20704c;">2IL<sub>in</sub>DL<sub>out</sub> + INDL<sub>out</sub><sup>2</sup></span>
                </div>
              
                <p><strong>Key Insight:</strong> In long chains-of-thought (CoTs), attention-related costs dominate. We define the ratio:</p>
                <div style="text-align: center; font-size: 18px; padding: 12px 0;">
                  Φ = [2rL<sub>in</sub>D + (rD + ID)L<sub>out</sub>] / 2P
                </div>
              
                <p>
                  When L<sub>out</sub> ≥ 4096, Φ can exceed 100–1000×, indicating that memory-bound attention dominates over parameter-bound compute.
                </p>
              </div>
          
              <span style="display: inline-block; margin-top: 6px; padding: 6px 10px; background-color: #e8f0fe; border-left: 4px solid #4b6cb7; border-radius: 4px; font-weight: 500;">
                <strong>Implication:</strong> As generation length increases, inference bottlenecks shift from linear <em>L<sub>out</sub>P</em> to quadratic <em>L<sub>out</sub><sup>2</sup>D</em> attention terms — eliciting the need for attention efficiency.
              </span>
            </p>
          </div>
          </details>
            
          <br>

          <h4 class="title is-5">
            <img src="static/images/icons/Stonks_emoji.png" style="height: 36px; display: inline; vertical-align: middle;"/>
            &nbsp; Kinetics Scaling Law
          </h4>
          <p>
            We compare the TTS scaling obtained from existing FLOPs-based analysis and our eFLOPs-based analysis. For each cost model, we identify the most optimal configuration for each individual task and average their accuracy scores under the same cost budget (FLOPs or eFLOPs).
            
            <div style="background: #ffffff; padding: 24px; font-size: 16px; line-height: 1.6; color: #333;">
              <figure>
                <img src="static/images/presenterationn-1.jpg" alt="Pareto Frontier Comparison" style="width: 100%; height: auto; margin: 0;" />
                <figcaption style="margin-top: 10px; text-align: center;">
                  <strong>Figure 3:</strong> Comparison of AIME24 pareto frontiers for long CoT-based Qwen3 TTS under FLOPs <strong>(ab)</strong> and eFLOPs <strong>(cd)</strong> cost models. Optimal model choices are highlighted in <strong>(ac)</strong>, while optimal CoT lengths are shown in <strong>(bd)</strong>. Similar observations hold for <em>Best-of-N</em> scaling.
                </figcaption>
              </figure>
            
            <span style="display: block; width: 100%; margin-top: 6px; padding: 6px 10px; background-color: #e8f0fe; border-left: 4px solid #4b6cb7; border-radius: 4px; font-weight: 500;">
              <strong>Takeaway:</strong>
              <ol>
                <li>
                  A FLOPs-based cost model tends to favor smaller models with more test-time compute (e.g., more trials or longer generations), until their accuracy gains begin to plateau.
                </li>
                <li>
                  In contrast, the eFLOPs-based model suggests that scaling up model size is more effective than increasing test-time compute—even in low-accuracy regimes.  We find that only beyond an emergent size (14B for Qwen3 series), longer CoT begin to outperform further parameter scaling. Therefore, even under limited budgets, it is often better to choose a larger model upfront.
                </li>
              </ol>
            </span>
            
            <div style="background: #ffffff; padding: 32px; font-size: 16px; line-height: 1.7; color: #333; border-radius: 8px; box-shadow: 0 2px 6px rgba(0,0,0,0.05);">

              <!-- add an expandable section here -->
              <details style="margin-bottom: 24px; border: 1px solid #ddd; border-radius: 6px; padding: 12px 16px; background-color: #f9f9fb;">
                <summary style="cursor: pointer; font-weight: 600; font-size: 16px; color: #2a63b2;">
                  Expand to understand why FLOPs-based and eFLOPs-based scaling laws diverge
                </summary>
              <div style="background: #ffffff; border-radius: 6px; padding: 24px; font-size: 16px; line-height: 1.6; color: #333;">
              <h5 class="title is-5" style="font-size: 20px; margin-bottom: 16px; color: #2a2a2a;">
                Why do they diverge?
              </h5>
            
              <p style="margin-bottom: 16px;">
                The <strong>“smallness” of smaller models</strong> is deceptive. As shown in <strong>Figure 4a</strong>, their KV cache footprint can be significantly large—even larger in proportion than that of much bigger models. This issue is exacerbated by the <strong>quadratic cost dependency on generation length</strong>, which the traditional FLOPs-based cost model fails to capture.
              </p>
            
              <p style="margin-bottom: 16px;">
                The plots below highlight this discrepancy through an <strong>iso-cost analysis</strong> showing how cost budgets influence the optimal combination of model size and CoT length.
                In <strong>Figure 4b</strong>, the <span style="color: #4b6cb7;"><strong>Iso-FLOPs contours</strong></span> are nearly vertical—indicating that model size plays a dominant role. In contrast, the <span style="color: #2f8f5b;"><strong>Iso-eFLOPs contours</strong></span> in <strong>Figure 4c</strong> appear more horizontal—revealing that optimal generation length is more responsive to cost budgets under the eFLOPs model.
              </p>
            
              <div style="display: flex; justify-content: space-between; flex-wrap: nowrap; gap: 20px; margin: 24px 0; overflow-x: auto;">
                <div style="flex: 0 0 32%; max-width: 32%; text-align: center; background: white; border-radius: 8px; padding: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06);">
                  <img src="static/images/kv_trend-1.jpg" alt="KV Trend" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 4a: KV memory trend across model sizes</p>
                </div>
                <div style="flex: 0 0 32%; max-width: 32%; text-align: center; background: white; border-radius: 8px; padding: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06);">
                  <img src="static/images/dotline1-1.jpg" alt="Iso-FLOPs" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 4b: Iso-FLOPs cost contours</p>
                </div>
                <div style="flex: 0 0 32%; max-width: 32%; text-align: center; background: white; border-radius: 8px; padding: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06);">
                  <img src="static/images/dotline2-1.jpg" alt="Iso-eFLOPs" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 4c: Iso-eFLOPs cost contours</p>
                </div>
              </div>
              
            
              <p>
                <span style="display: block; background-color: #e6f0fb; border-left: 4px solid #4b6cb7; padding: 12px 16px; border-radius: 6px; margin-bottom: 16px;">
                  <strong>KV memory scales sublinearly with model size:</strong>
                  Smaller models can have substantial memory footprints due to disproportionately large KV caches. For instance, Qwen3-0.6B needs 3.5GB of KV cache for 32K tokens, while Qwen3-32B uses only 8GB. Empirically, doubling parameters increases KV memory by only ~1.2×.
                </span>
            
                <span style="display: block; background-color: #e4f7ec; border-left: 4px solid #2f8f5b; padding: 12px 16px; border-radius: 6px;">
                  <strong>Quadratic cost penalizes long generations:</strong>
                  Under the <em>L<sup>2</sup>D</em> model, generation cost grows faster than model size. As a result, small models can no longer compensate for limited capacity by simply generating longer outputs—especially for complex reasoning tasks.
                </span>
              </p>
            </div>
            </details>
            
          </div>
        </div>
      </div>
    </div>
</section>
  
<section>
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: center;">
          <img src="static/images/icons/sparse_bolt.png" style="height: 64px; display: inline; vertical-align: middle;"/>
          &nbsp; Kinetics Sparse Test-Time Scaling
        </h2>
        <div class="content has-text-justified">
          <p>
            Our cost analysis hints at the importance of attention efficiency in TTS. Our sparse scaling law studies the effectiveness of attention sparsity and how it can further reshape the dense TTS pareto frontier. <em>To see the full potential of sparse attention, we first study <strong>oracle top-k attention</strong> with zero search overhead to simplify the analysis.</em>
            <h4 style="margin-bottom: 16px;">
              <img src="static/images/icons/Idea.png" style="height: 36px; display: inline; vertical-align: middle;"/>
              &nbsp;Sparse attention significantly enhances problem-solving performance
            </h4>
            <div style="background: #ffffff; padding: 24px; font-size: 16px; line-height: 1.6; color: #333;">
              <!-- 6 figures (in 2x3 grid) , center aligned -->
              <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
                <div style="text-align: center;">
                  <img src="static/images/aime24_acc_vs_trials.jpg" alt="topk-benefits" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 5a: Best-of-N scaling comparison between oracle top-k and dense attention</p>
                </div>
                <div style="text-align: center;">
                  <img src="static/images/aime24_qwen3-8b_scaling_logerr_extended_tradeoff-1.jpg" alt="topk-benefits" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 5b: Best-of-N using Qwen3-8B </p>
                </div>
                <div style="text-align: center;">
                  <img src="static/images/aime24_qwen3-32b_scaling_logerr_extended_tradeoff-1.jpg" alt="topk-benefits" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 5c: Best-of-N using Qwen3-32B </p>
                </div>
                <div style="text-align: center;">
                  <img src="static/images/aime24_acc_vs_genlen-1.jpg" alt="topk-benefits" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 5d: Long-CoT scaling comparison between oracle top-k and dense attention</p>
                </div>
                <div style="text-align: center;">
                  <img src="static/images/aime24_qwen3-8b_acc_vs_genlen-1.jpg" alt="topk-benefits" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 5e: Long-CoT using Qwen3-8B </p>
                </div>
                <div style="text-align: center;">
                  <img src="static/images/aime24_qwen3-32b_acc_vs_genlen-1.jpg" alt="topk-benefits" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 5f: Long-CoT using Qwen3-32B </p>
                </div>
              </div>
              <br>
              <p>
                <strong>Figure 5: Figures 5a and 5d</strong> show that sparse attention significantly improves the cost-accuracy trade-off, enabling higher accuracy at lower compute budgets. 
                <strong>Figures 5b, 5c, 5e, and 5f</strong> highlight that both 8B and 32B models benefit from sparsity, with <em>50–60 percentage point</em> gains in low-cost settings and consistent <em>~5 point</em> improvements even in high-cost regimes. 
                Notably, sparse models reach these performance levels at much lower costs —for context, 10<sup>5</sup> Tera-eFLOPs is just 22 seconds of B200 usage.
              </p>
            </div>
            <br>
            <h4 style="margin-bottom: 16px;">
              <img src="static/images/icons/Idea.png" style="height: 36px; display: inline; vertical-align: middle;"/>
              &nbsp; Can sparse attention reshape the Kinetics scaling law?
            </h4>
            
            <div style="background: #ffffff; padding: 24px; font-size: 16px; line-height: 1.6; color: #333; border-radius: 6px;">
              <div style="display: flex; align-items: flex-start; gap: 24px; flex-wrap: wrap;">
                <div style="flex: 1 1 55%;">
                  
                  <p>
                    <!-- We saw before that as model size increases, the <strong>KV memory footprint grows slowly</strong>, making larger models relatively more efficient in memory usage.
                    However, with <strong>KV sparsification</strong>, the criticality of this memory footprint is relaxed as the <strong style="color: #2f8f5b;">optimal KV cache budget increases very gradually with cost budget</strong>.  -->
                    To see how big of a role attention sparsity can have, we include KV sparsification into the list of TTS variables. Our trade-off analysis between KV sparsity and other TTS variables like CoT length and number of trials reveals that,
                    <p>
                    <span style="display: block; background-color: #e6f0fb; border-left: 4px solid #4b6cb7; padding: 12px 16px; border-radius: 6px;">
                      With increasing cost budget, it is more beneficial to scale generation tokens than KV cache budget.
                    </span>
                    </p>
                    In other words, a smaller KV cache budget is usually sufficient for satisfactory performance. Furthermore, <strong style="color: #2f8f5b;">it allows us to generate more tokens at the same cost budget</strong>. Importantly, the original <em>quadratic</em> cost scaling with generation length is replaced by a <em>linear dependency</em>, significantly reducing cost for longer generations.
                  </p>
                </div>
                <div style="flex: 1 1 40%; text-align: center;">
                  <img src="static/images/presenterationn7-1.jpg" alt="topk-benefits" style="width: 100%; height: auto; border-radius: 6px" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666;">Figure 6: Compared to the dense scaling, small models (0.6B, 1.7B, 4B) are more effective with sparse attention. In other words, they occupy more space in the Pareto Frontier</p>
                </div>
              </div>
            </div>   
            <br>
            <h4 style="margin-bottom: 16px;">
              <img src="static/images/icons/MathematicsCompass.png" style="height: 36px; display: inline; vertical-align: middle;"/>
              &nbsp; Exploring tractable sparse attention algorithms
            </h4>
            <div style="background: #ffffff; padding: 24px; font-size: 16px; line-height: 1.6; color: #333;">
              <p>
                We explore two tractable alternatives to oracle top-k attention: <strong>block top-k attention</strong> and <strong>sliding window attention</strong>. Although sliding window attention is easier to implement and has zero search overhead, its performance is extremely poor. Block top-k attention strikes a good balance between performance and efficiency. 
              </p>
            
              <!-- Wrap each image + caption in a single div -->
              <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
                <div>
                  <img src="static/images/aime24_qwen3-8b_sparse_method_comparison-1.jpg" alt="sparse comparison trials" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666; text-align: center;">Figure 7a: Sparse algorithm comparison</p>
                </div>
                <div>
                  <img src="static/images/aime24_acc_vs_trials_block_topk_trials-1.jpg" alt="block topk trial" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666; text-align: center;">Figure 7b: Block top-k Best-of-N scaling</p>
                </div>
                <div>
                  <img src="static/images/aime24_qwen3-8b_sparse_method_comparison_genlen-1.jpg" alt="block topk genlen" style="width: 100%; height: auto; border-radius: 6px;" />
                  <p style="margin-top: 8px; font-size: 14px; color: #666; text-align: center;">Figure 7c: Block top-k Long-CoT scaling</p>
                </div>
              </div>
            
              <p style="margin-top: 20px;">
                <strong>Figure 7:</strong> <strong>(a)</strong> illustrates how block top-k attention scaling closely follows the oracle top-k scaling for the Qwen3-8B model on AIME24 tasks. <strong>(b)</strong> and <strong>(c)</strong> illustrate the Pareto frontiers of block top-k attention.
              </p>
            </div>
            
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section: Conclusion and Future Work -->
<section class="section hero is-light">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: center;">
          <img src="static/images/icons/Telescope.png" style="height: 50px; display: inline; vertical-align: middle;"/>
          &nbsp; Conclusion and Future Work
        </h2>
        <div class="content has-text-justified">
          <p>
            In this post, we introduced the Kinetics Scaling Law, emphasizing that attention cost, not parameter count, is the dominant factor at test time, fundamentally reshaping the previous scaling law. We further demonstrated that sparse attention is crucial for achieving more effective and scalable test-time scaling. While our discussion focused on a simple sparse attention algorithm, block top-k attention, we anticipate that more advanced algorithms will approach or even outperform oracle top-k scaling. Moreover, sparse attention drastically reduces inference cost, enabling more reasoning trials and longer generations. This unlocks greater flexibility in configuring TTS strategies within a fixed resource. This work aims to contribute to the understanding of efficiency and scalability challenges in the test-time scaling era, spanning model architecture, system-level implementation, and hardware design. We highlight the central role of sparsity in addressing these challenges.
          </p>
        </div>
        <div class="has-text-centered">
          <img src="static/images/icons/Kinetics.png" alt="<i>TriForce</i>" width="300" height="300" />
        </div>
      </div>
    </div>
  </div>
</section>
  
  
  <!-- Section: References -->
  <section class="section" id="BibTeX">
    <!-- <div class="container is-max-desktop content"> -->
      <div class="container is-fluid">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{chen2024magicdecbreakinglatencythroughputtradeoff,
              title={Kinetics: Rethinking Test-Time Scaling Laws}, 
              author={Ranajoy Sadhukhan and Zhuoming Chen and Yang Zhou and Emma Strubell and Beidi Chen},
              year={2024},
              eprint={2408.11049},
              archivePrefix={arXiv},
              primaryClass={cs.CL},
              url={https://arxiv.org/abs/2408.11049}, 
        }</code></pre>
        </div>
      </div>
    </div>
  </section>
  
  <footer class="footer">
    <div class="container is-fluid">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>. The icons are created by GPT4. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
